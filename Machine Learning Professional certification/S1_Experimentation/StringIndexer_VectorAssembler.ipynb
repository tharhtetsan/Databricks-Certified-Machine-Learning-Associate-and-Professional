{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba4c6d7-bd90-4926-99ec-f104deee411c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d61f457-9023-49ed-a4f7-3847aa3d905c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Example\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27936c6a-10ab-49e7-bad3-c9e2d776295c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n+-------+------+---+------+\n|   Name|Gender|Age|Salary|\n+-------+------+---+------+\n|  Alice|Female| 29|  3000|\n|    Bob|  Male| 34|  4000|\n|Charlie|Female| 25|  3500|\n|  David|  Male| 40|  4500|\n|    Eve|Female| 30|  3200|\n+-------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample data in a list format\n",
    "data = [\n",
    "    (\"Alice\", \"Female\", 29, 3000),\n",
    "    (\"Bob\", \"Male\", 34, 4000),\n",
    "    (\"Charlie\", \"Female\", 25, 3500),\n",
    "    (\"David\", \"Male\", 40, 4500),\n",
    "    (\"Eve\", \"Female\", 30, 3200)\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"Name\", \"Gender\", \"Age\", \"Salary\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original Data:\")\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c937c16-d48b-4af2-b849-03cd52983361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After StringIndexer (Gender -> GenderIndex):\n+-------+------+---+------+-----------+\n|   Name|Gender|Age|Salary|GenderIndex|\n+-------+------+---+------+-----------+\n|  Alice|Female| 29|  3000|        0.0|\n|    Bob|  Male| 34|  4000|        1.0|\n|Charlie|Female| 25|  3500|        0.0|\n|  David|  Male| 40|  4500|        1.0|\n|    Eve|Female| 30|  3200|        0.0|\n+-------+------+---+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Index the \"Gender\" column (convert categorical data to numerical)\n",
    "indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"GenderIndex\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "print(\"After StringIndexer (Gender -> GenderIndex):\")\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04c9751-0dc6-47be-809a-9ca8efa19782",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.daemon.driver.DriverClientDestroyedException: abort: DriverClient destroyed\n",
       "\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$4(DriverClient.scala:755)\n",
       "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:54)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:15)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:273)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:269)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:6)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:15)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:14)\n",
       "\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n",
       "\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:477)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:829)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "com.databricks.backend.daemon.driver.DriverClientDestroyedException: abort: DriverClient destroyed\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$4(DriverClient.scala:755)\n\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:54)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:15)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:273)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:269)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:6)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:15)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:14)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:477)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: One-hot encode the \"GenderIndex\" column\n",
    "encoder = OneHotEncoder(inputCol=\"GenderIndex\", outputCol=\"GenderVec\")\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "print(\"After OneHotEncoder (GenderIndex -> GenderVec):\")\n",
    "df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593c4574-99dc-4d86-b586-09db1658c020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Assemble the features (Age, Salary, and GenderVec) into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Age\", \"Salary\", \"GenderVec\"],  # Input features\n",
    "    outputCol=\"features\"  # Output column with assembled vector\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_encoded)\n",
    "\n",
    "print(\"After VectorAssembler (Assembled Features):\")\n",
    "df_final.select(\"Name\", \"features\").show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f129cfc-2c8c-4f1b-ad48-647f4b3205c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "StringIndexer_VectorAssembler",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
